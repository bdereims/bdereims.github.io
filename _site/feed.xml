<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://bdereims.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bdereims.github.io/" rel="alternate" type="text/html" /><updated>2022-09-02T11:47:39+02:00</updated><id>https://bdereims.github.io/feed.xml</id><title type="html">bdereims’ hacks</title><subtitle>This is my personal blog which references all my hacks. Use as is without any warranty. If there are typos and disclosing information, let me know.</subtitle><entry><title type="html">Run NGC container in Windows 11</title><link href="https://bdereims.github.io/conatiner/ia/ngc/2022/07/21/ngc-container-wsl.html" rel="alternate" type="text/html" title="Run NGC container in Windows 11" /><published>2022-07-21T15:37:15+02:00</published><updated>2022-07-21T15:37:15+02:00</updated><id>https://bdereims.github.io/conatiner/ia/ngc/2022/07/21/ngc-container-wsl</id><content type="html" xml:base="https://bdereims.github.io/conatiner/ia/ngc/2022/07/21/ngc-container-wsl.html"><![CDATA[<p>I’m a big server Linux fan and have used laptop macOS for over 10 years now for its unix like shell. Windows has now the famous Windows Subsystem for Linux, time to give it a shot! Furthermore, I want also escape Docker Desktop fees… Let’s try something fancy.</p>

<p>First step is to activate WSL on your Win11 box. So, to do that you need to enable 2 features like below:</p>

<p><img src="/gfx/activate-wsl.png" alt="win11-wsl" /></p>

<p>Followed in command prompt: <code class="language-plaintext highlighter-rouge">wsl --install --distribution Ubuntu-20.04</code></p>

<p>To run containers, we need to install podman. It’s a free open source docker engine alternative. If you hope to use docker-ce in wsl you wrong, it needs Docker Desktop that is not free if you’re working for large enterprise.</p>

<p>Within WSL shell:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># source /etc/os-release
# sh -c "echo 'deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/ /' &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list"
# wget -nv https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/xUbuntu_${VERSION_ID}/Release.key -O- | apt-key add -
# apt-get update -qq -y
# apt-get -qq --yes install podman
# podman version
</code></pre></div></div>

<p>You should get:</p>

<p><img src="/gfx/podam-version.png" alt="podman-version" /></p>

<p>You have to set default registry: 
<code class="language-plaintext highlighter-rouge"># echo 'unqualified-search-registries = ["docker.io"]' | sudo tee /etc/containers/registries.conf</code></p>

<p>Verify that nvidia driver is working propely: <code class="language-plaintext highlighter-rouge"># nvidia-smi</code>, you should get something like:</p>

<p><img src="/gfx/nvidia-smi-win11.png" alt="nvidia-smi" /></p>

<p>Install <a href="https://github.com/henrymai/podman_wsl2_cuda_rootless">nvidia-container-toolkit</a> (cf. https://github.com/henrymai/podman_wsl2_cuda_rootless )</p>

<p>Modify file <code class="language-plaintext highlighter-rouge">/etc/nvidia-container-runtime/config.toml</code> like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>disable-require = false
#swarm-resource = "DOCKER_RESOURCE_GPU"
#accept-nvidia-visible-devices-envvar-when-unprivileged = true
#accept-nvidia-visible-devices-as-volume-mounts = false

[nvidia-container-cli]
#root = "/run/nvidia/driver"
#path = "/usr/bin/nvidia-container-cli"
environment = []
#debug = "/var/log/nvidia-container-toolkit.log"
#ldcache = "/etc/ld.so.cache"
load-kmods = true
#no-cgroups = false
no-cgroups = true
#user = "root:video"
ldconfig = "@/sbin/ldconfig.real"

[nvidia-container-runtime]
#debug = "/var/log/nvidia-container-runtime.log"
debug = "~/.local/nvidia-container-runtime.log"
log-level = "info"

# Specify the runtimes to consider. This list is processed in order and the PATH
# searched for matching executables unless the entry is an absolute path.
runtimes = [
    "docker-runc",
    "runc",
]

mode = "auto"

[nvidia-container-runtime.modes.csv]
mount-spec-path = "/etc/nvidia-container-runtime/host-files-for-container.d"
</code></pre></div></div>

<p>Configure hook with this lines:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Configure hook

sudo -i
mkdir -p /usr/share/containers/oci/hooks.d/
cat &lt;&lt; EOF | sudo tee /usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
{
    "version": "1.0.0",
    "hook": {
        "path": "/usr/bin/nvidia-container-toolkit",
        "args": ["nvidia-container-toolkit", "prestart"],
        "env": [
            "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
        ]
    },
    "when": {
        "always": true,
        "commands": [".*"]
    },
    "stages": ["prestart"]
}
EOF
</code></pre></div></div>

<p>Last step for configuration is to increase limits defined in <code class="language-plaintext highlighter-rouge">/etc/security/limits.conf</code> like below, replace <code class="language-plaintext highlighter-rouge">[your-user]</code> by what returns <code class="language-plaintext highlighter-rouge">whoami</code>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[your-user] soft memlock unlimited
[your-user] hard memlock unlimited
[your-user] soft stack 65536
[your-user] hard stack 65536
</code></pre></div></div>

<p>If you not yet familiar with that is NGC, it’s time to visite <a href="https://catalog.ngc.nvidia.com/">NGC Catalog</a> and <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/collections/gettingstarted">the getting started</a>. Create your account and install <a href="https://ngc.nvidia.com/setup">ngc cli</a>.</p>

<p>Let’s try this <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/resources/fashion_mnist_tf_example/version/1.0/files/FashionMNIST%20Notebook.ipynb">example</a>.</p>

<p>Launch example with: <code class="language-plaintext highlighter-rouge">podman run -it --gpus all -p 8888:8888 -v $PWD:/projects --network=host nvcr.io/nvidia/tensorflow:20.08-tf1-py3</code> and follow jupyter notebook. You should able to run example:</p>

<p><img src="/gfx/ngc-example.png" alt="ngc-example" /></p>

<p>Conclusion:</p>
<ul>
  <li>podman on wsl works great</li>
  <li>with ngc container running localy, it’s easy to learn and test ML/AI topics</li>
  <li>first step before to work with large scale environnement like Base Command</li>
</ul>]]></content><author><name></name></author><category term="conatiner" /><category term="ia" /><category term="ngc" /><summary type="html"><![CDATA[I’m a big server Linux fan and have used laptop macOS for over 10 years now for its unix like shell. Windows has now the famous Windows Subsystem for Linux, time to give it a shot! Furthermore, I want also escape Docker Desktop fees… Let’s try something fancy.]]></summary></entry><entry><title type="html">Ubunru 20.04 LTS Jumpbox</title><link href="https://bdereims.github.io/linux/ubuntu/vm/cloud-garage/2022/07/19/jumpbox.html" rel="alternate" type="text/html" title="Ubunru 20.04 LTS Jumpbox" /><published>2022-07-19T10:00:00+02:00</published><updated>2022-07-19T10:00:00+02:00</updated><id>https://bdereims.github.io/linux/ubuntu/vm/cloud-garage/2022/07/19/jumpbox</id><content type="html" xml:base="https://bdereims.github.io/linux/ubuntu/vm/cloud-garage/2022/07/19/jumpbox.html"><![CDATA[<p>It’s always good to have a VM Jumbox or a simple way to configure your machine! For this I wrote a bunch of automated scripts, for this annoying task. You will get the necessary setup and tools to perform your daily tasks. It comes with: kubectl, stern, docker engine, docker-compose, helm, govc, kubectx, kubens, extend-rootfs.sh, curl, jq, sshpass, unzip, tmux, bash-completion, startup-script.</p>

<p>Use this <a href="https://github.com/bdereims/cloud-garage/tree/main/install/sixty9">repo</a> and execute as root <code class="language-plaintext highlighter-rouge">bootsrap.sh</code>. Only validated for freshly installed Ubuntu 20.04 LTS, as VM or bare metal.</p>

<p>Enjoy!</p>]]></content><author><name></name></author><category term="linux" /><category term="ubuntu" /><category term="vm" /><category term="cloud-garage" /><summary type="html"><![CDATA[It’s always good to have a VM Jumbox or a simple way to configure your machine! For this I wrote a bunch of automated scripts, for this annoying task. You will get the necessary setup and tools to perform your daily tasks. It comes with: kubectl, stern, docker engine, docker-compose, helm, govc, kubectx, kubens, extend-rootfs.sh, curl, jq, sshpass, unzip, tmux, bash-completion, startup-script.]]></summary></entry><entry><title type="html">Nested SDDC for Testing/Lab/Demo</title><link href="https://bdereims.github.io/vmware/sdd/vsphere/ovh/2022/06/24/cPodFactory.html" rel="alternate" type="text/html" title="Nested SDDC for Testing/Lab/Demo" /><published>2022-06-24T19:00:00+02:00</published><updated>2022-06-24T19:00:00+02:00</updated><id>https://bdereims.github.io/vmware/sdd/vsphere/ovh/2022/06/24/cPodFactory</id><content type="html" xml:base="https://bdereims.github.io/vmware/sdd/vsphere/ovh/2022/06/24/cPodFactory.html"><![CDATA[<p>cPodFactory is one of my projects during my VMware years. The main purpose is to automatically and dynamically create nested SDDC environments. It’s means you have the capability to create a separated but nested vSphere SDDC fully routed (dedicated L2 but L3 routed) configured and ready to use.</p>

<p>It comes with <code class="language-plaintext highlighter-rouge">cpodctl</code> command line giving you all necessary command to manage cPod lifecycle. A cPod is a lab or if you prefer the nested vSphere. You can:</p>
<ol>
  <li>Create a cPod: naked one (without esx but all networking facilities) or specifying number of esx</li>
  <li>Delete a cPod entirely, caution - it’s fast without roll back</li>
  <li>Deploy &amp; configure vCenter accordingly your cPod</li>
  <li>List all cPods</li>
  <li>Deploy VCF cloudbuilder if you want to transform cPod into VCF cluster(s)</li>
  <li>Retrieve the auto generated cPod password</li>
  <li>Override some variables to customize your cPod</li>
  <li>Use completion in bash to facilitate life with tab key</li>
</ol>

<p>cpodctl cli:
<img src="/gfx/cpodfactory-cpodctl.png" alt="cpodctl cli" /></p>

<p>A large Availability Zone running physical layer by VCF:
<img src="/gfx/cpodfactory-az-fkd.png" alt="cpodctl cli" /></p>

<p>cPodFactory also offers:</p>
<ul>
  <li>Multi availability zones and connect them via wireguard: it’s possible to move workload across different az or to connect services on different cPods</li>
  <li>DNS resolution for all az</li>
  <li>VPN for client with OpenVPN, Windows/Mac/Linux compatible</li>
  <li>DHCP for each L2 connectivity</li>
  <li>Dynamic L3 routing with BPG, with the possibility to peer NSX Edge in a cPod</li>
  <li>Nested NSX and nested VSAN</li>
</ul>

<p>It needs for the physical layer:</p>
<ul>
  <li>at least 2 or 3 esx 6.7 + vCenter or VCF with 4 esx</li>
  <li>vSAN for storage or any shared datastore (NFS, iSCSI, FC)</li>
  <li>VLAN backend network or NSX overlay (NSX is not mandatory at all)</li>
</ul>

<p>I ran it for 4 years to help colleagues to have serious labs and also used it for all my customer workshops. We have never had severe issue of availability or performance - thanks to vSphere for its capabilities to run heavy workloads. It has been deployed in several country interconnected via wireguard: Paris(FR), OVH(FR), Madrid(SP), Munich(DE), Dubai(UAE).</p>

<p>To give you some ideas of use cases:</p>
<ul>
  <li>Large Proof Of Concept with Tanzu running Telco NVFs and consuming +480Gb memory in kubernetes, 9 esx cPod</li>
  <li>Demo cPod dedicated to vRealize Automation with NSX and vSAN connected with Public Cloud deploying hybrid App</li>
  <li>multi VCF cPods in different regions (Paris &lt;-&gt; Dubai) moving VM workloads via HCX</li>
  <li>Cloud Migration from cPod in OVH to VMC in AWS, moving VM via HCX, fully routed without application interruption, keeping on-prem DB</li>
  <li>All new vSphere 8 cPod with Tanzu Grid 2.0 for training purpose</li>
  <li>nVIDIA AI Enterprise naked cPod bringing agile AI for enterprise running on vSphere</li>
</ul>

<p>This project is open source. You can use it, improve it but not in production. Nested is not supported/allowed for production.</p>

<p><a href="https://github.com/bdereims/cPodFactory">Check out git repo</a> - you find here all the necessary to install yours.</p>

<p>Enjoy!</p>]]></content><author><name></name></author><category term="vmware" /><category term="sdd" /><category term="vsphere" /><category term="ovh" /><summary type="html"><![CDATA[cPodFactory is one of my projects during my VMware years. The main purpose is to automatically and dynamically create nested SDDC environments. It’s means you have the capability to create a separated but nested vSphere SDDC fully routed (dedicated L2 but L3 routed) configured and ready to use.]]></summary></entry></feed>