<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://bdereims.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://bdereims.github.io/" rel="alternate" type="text/html" /><updated>2022-09-02T12:55:41+02:00</updated><id>https://bdereims.github.io/feed.xml</id><title type="html">bdereims’ hacks</title><subtitle>This is my personal blog which references all my hacks. Use as is without any warranty. If there are typos and disclosing information, let me know.</subtitle><entry><title type="html">OCI - one more Cloud</title><link href="https://bdereims.github.io/oracle/2022/09/02/oci.html" rel="alternate" type="text/html" title="OCI - one more Cloud" /><published>2022-09-02T10:00:00+02:00</published><updated>2022-09-02T10:00:00+02:00</updated><id>https://bdereims.github.io/oracle/2022/09/02/oci</id><content type="html" xml:base="https://bdereims.github.io/oracle/2022/09/02/oci.html"><![CDATA[<p>For an interne project I have to use <a href="https://www.oracle.com/cloud/">Oracle Cloud Infrastructure</a>. I was very skeptical about it and I’m wondering why OCI?
Anyway, it was time to discover what it is and automate things to avoid to do again boring tasks.
Today there are so many Infrastructure as Code solutions (pulumi, terraform and others) but it always good to go with  <code class="language-plaintext highlighter-rouge">The Hard Way</code> to discover a solution. This is why I decided to write some bash scripts executing <a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/cliconcepts.htm">oci cli</a> commands covering my needs:</p>
<ul>
  <li>create a compartment, it’s like a tenant</li>
  <li>create VCN, like a VPC</li>
  <li>create Public &amp; Private Subnets</li>
  <li>create internet gateway</li>
  <li>create ingress firewall rules</li>
  <li>create instance of my Linux sixty9 jumpBox, configure iptables, wireguard VPN and DNS</li>
  <li>possibility to deploy VMs as master/worker kubernetes nodes and bare metal worker kubernetes node with GPU in same VCN</li>
  <li>use kubespray to setup kubernetes from sixty9</li>
  <li>obviously destroy all the things</li>
</ul>

<p>These scripts give me flexibility to test things, to add nodes when it’s needed. A perfect use case for me. I share it with you in a <a href="https://github.com/bdereims/cloud-garage/tree/main/install/oci">git repo</a> a subpart of <a href="https://github.com/bdereims/cloud-garage/tree/main/install">cloud-garage</a> repo.</p>

<p>OCI portal:
<img src="/gfx/oci-instances.png" alt="OCI Portal" /></p>

<p>VCN configuration:
<img src="/gfx/oci-vcn.png" alt="VCN" /></p>

<p>With wireguard, I can directly interact with kubernetes from my laptop without exposing it publicly:
<img src="/gfx/oci-kubectl.png" alt="VCN" /></p>

<p>Finally OCI is an excellent solution. I don’t regret discovering Oracle Cloud Infrastructure.</p>

<p>Enjoy!</p>]]></content><author><name></name></author><category term="oracle" /><summary type="html"><![CDATA[For an interne project I have to use Oracle Cloud Infrastructure. I was very skeptical about it and I’m wondering why OCI? Anyway, it was time to discover what it is and automate things to avoid to do again boring tasks. Today there are so many Infrastructure as Code solutions (pulumi, terraform and others) but it always good to go with The Hard Way to discover a solution. This is why I decided to write some bash scripts executing oci cli commands covering my needs: create a compartment, it’s like a tenant create VCN, like a VPC create Public &amp; Private Subnets create internet gateway create ingress firewall rules create instance of my Linux sixty9 jumpBox, configure iptables, wireguard VPN and DNS possibility to deploy VMs as master/worker kubernetes nodes and bare metal worker kubernetes node with GPU in same VCN use kubespray to setup kubernetes from sixty9 obviously destroy all the things]]></summary></entry><entry><title type="html">Run NGC container in Windows 11</title><link href="https://bdereims.github.io/ia/2022/07/21/ngc-container-wsl.html" rel="alternate" type="text/html" title="Run NGC container in Windows 11" /><published>2022-07-21T15:37:15+02:00</published><updated>2022-07-21T15:37:15+02:00</updated><id>https://bdereims.github.io/ia/2022/07/21/ngc-container-wsl</id><content type="html" xml:base="https://bdereims.github.io/ia/2022/07/21/ngc-container-wsl.html"><![CDATA[<p>I’m a big server Linux fan and have used laptop macOS for over 10 years now for its unix like shell. Windows has now the famous Windows Subsystem for Linux, time to give it a shot! Furthermore, I want also escape Docker Desktop fees… Let’s try something fancy.</p>

<p>First step is to activate WSL on your Win11 box. So, to do that you need to enable 2 features like below:</p>

<p><img src="/gfx/activate-wsl.png" alt="win11-wsl" /></p>

<p>Followed in command prompt: <code class="language-plaintext highlighter-rouge">wsl --install --distribution Ubuntu-20.04</code></p>

<p>To run containers, we need to install podman. It’s a free open source docker engine alternative. If you hope to use docker-ce in wsl you wrong, it needs Docker Desktop that is not free if you’re working for large enterprise.</p>

<p>Within WSL shell:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># source /etc/os-release
# sh -c "echo 'deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/xUbuntu_${VERSION_ID}/ /' &gt; /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list"
# wget -nv https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/xUbuntu_${VERSION_ID}/Release.key -O- | apt-key add -
# apt-get update -qq -y
# apt-get -qq --yes install podman
# podman version
</code></pre></div></div>

<p>You should get:</p>

<p><img src="/gfx/podam-version.png" alt="podman-version" /></p>

<p>You have to set default registry: 
<code class="language-plaintext highlighter-rouge"># echo 'unqualified-search-registries = ["docker.io"]' | sudo tee /etc/containers/registries.conf</code></p>

<p>Verify that nvidia driver is working propely: <code class="language-plaintext highlighter-rouge"># nvidia-smi</code>, you should get something like:</p>

<p><img src="/gfx/nvidia-smi-win11.png" alt="nvidia-smi" /></p>

<p>Install <a href="https://github.com/henrymai/podman_wsl2_cuda_rootless">nvidia-container-toolkit</a> (cf. https://github.com/henrymai/podman_wsl2_cuda_rootless )</p>

<p>Modify file <code class="language-plaintext highlighter-rouge">/etc/nvidia-container-runtime/config.toml</code> like this:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>disable-require = false
#swarm-resource = "DOCKER_RESOURCE_GPU"
#accept-nvidia-visible-devices-envvar-when-unprivileged = true
#accept-nvidia-visible-devices-as-volume-mounts = false

[nvidia-container-cli]
#root = "/run/nvidia/driver"
#path = "/usr/bin/nvidia-container-cli"
environment = []
#debug = "/var/log/nvidia-container-toolkit.log"
#ldcache = "/etc/ld.so.cache"
load-kmods = true
#no-cgroups = false
no-cgroups = true
#user = "root:video"
ldconfig = "@/sbin/ldconfig.real"

[nvidia-container-runtime]
#debug = "/var/log/nvidia-container-runtime.log"
debug = "~/.local/nvidia-container-runtime.log"
log-level = "info"

# Specify the runtimes to consider. This list is processed in order and the PATH
# searched for matching executables unless the entry is an absolute path.
runtimes = [
    "docker-runc",
    "runc",
]

mode = "auto"

[nvidia-container-runtime.modes.csv]
mount-spec-path = "/etc/nvidia-container-runtime/host-files-for-container.d"
</code></pre></div></div>

<p>Configure hook with this lines:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Configure hook

sudo -i
mkdir -p /usr/share/containers/oci/hooks.d/
cat &lt;&lt; EOF | sudo tee /usr/share/containers/oci/hooks.d/oci-nvidia-hook.json
{
    "version": "1.0.0",
    "hook": {
        "path": "/usr/bin/nvidia-container-toolkit",
        "args": ["nvidia-container-toolkit", "prestart"],
        "env": [
            "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
        ]
    },
    "when": {
        "always": true,
        "commands": [".*"]
    },
    "stages": ["prestart"]
}
EOF
</code></pre></div></div>

<p>Last step for configuration is to increase limits defined in <code class="language-plaintext highlighter-rouge">/etc/security/limits.conf</code> like below, replace <code class="language-plaintext highlighter-rouge">[your-user]</code> by what returns <code class="language-plaintext highlighter-rouge">whoami</code>:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[your-user] soft memlock unlimited
[your-user] hard memlock unlimited
[your-user] soft stack 65536
[your-user] hard stack 65536
</code></pre></div></div>

<p>If you not yet familiar with that is NGC, it’s time to visite <a href="https://catalog.ngc.nvidia.com/">NGC Catalog</a> and <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/collections/gettingstarted">the getting started</a>. Create your account and install <a href="https://ngc.nvidia.com/setup">ngc cli</a>.</p>

<p>Let’s try this <a href="https://catalog.ngc.nvidia.com/orgs/nvidia/resources/fashion_mnist_tf_example/version/1.0/files/FashionMNIST%20Notebook.ipynb">example</a>.</p>

<p>Launch example with: <code class="language-plaintext highlighter-rouge">podman run -it --gpus all -p 8888:8888 -v $PWD:/projects --network=host nvcr.io/nvidia/tensorflow:20.08-tf1-py3</code> and follow jupyter notebook. You should able to run example:</p>

<p><img src="/gfx/ngc-example.png" alt="ngc-example" /></p>

<p>Conclusion:</p>
<ul>
  <li>podman on wsl works great</li>
  <li>with ngc container running localy, it’s easy to learn and test ML/AI topics</li>
  <li>first step before to work with large scale environnement like Base Command</li>
</ul>]]></content><author><name></name></author><category term="ia" /><summary type="html"><![CDATA[I’m a big server Linux fan and have used laptop macOS for over 10 years now for its unix like shell. Windows has now the famous Windows Subsystem for Linux, time to give it a shot! Furthermore, I want also escape Docker Desktop fees… Let’s try something fancy.]]></summary></entry><entry><title type="html">Ubunru 20.04 LTS Jumpbox</title><link href="https://bdereims.github.io/linux/2022/07/19/jumpbox.html" rel="alternate" type="text/html" title="Ubunru 20.04 LTS Jumpbox" /><published>2022-07-19T10:00:00+02:00</published><updated>2022-07-19T10:00:00+02:00</updated><id>https://bdereims.github.io/linux/2022/07/19/jumpbox</id><content type="html" xml:base="https://bdereims.github.io/linux/2022/07/19/jumpbox.html"><![CDATA[<p>It’s always good to have a VM Jumbox or a simple way to configure your machine! For this I wrote a bunch of automated scripts, for this annoying task. You will get the necessary setup and tools to perform your daily tasks. It comes with: kubectl, stern, docker engine, docker-compose, helm, govc, kubectx, kubens, extend-rootfs.sh, curl, jq, sshpass, unzip, tmux, bash-completion, startup-script.</p>

<p>Use this <a href="https://github.com/bdereims/cloud-garage/tree/main/install/sixty9">repo</a> and execute as root <code class="language-plaintext highlighter-rouge">bootsrap.sh</code>. Only validated for freshly installed Ubuntu 20.04 LTS, as VM or bare metal.</p>

<p>Enjoy!</p>]]></content><author><name></name></author><category term="linux" /><summary type="html"><![CDATA[It’s always good to have a VM Jumbox or a simple way to configure your machine! For this I wrote a bunch of automated scripts, for this annoying task. You will get the necessary setup and tools to perform your daily tasks. It comes with: kubectl, stern, docker engine, docker-compose, helm, govc, kubectx, kubens, extend-rootfs.sh, curl, jq, sshpass, unzip, tmux, bash-completion, startup-script.]]></summary></entry><entry><title type="html">Nested SDDC for Testing/Lab/Demo</title><link href="https://bdereims.github.io/vmware/2022/06/24/cPodFactory.html" rel="alternate" type="text/html" title="Nested SDDC for Testing/Lab/Demo" /><published>2022-06-24T19:00:00+02:00</published><updated>2022-06-24T19:00:00+02:00</updated><id>https://bdereims.github.io/vmware/2022/06/24/cPodFactory</id><content type="html" xml:base="https://bdereims.github.io/vmware/2022/06/24/cPodFactory.html"><![CDATA[<p>cPodFactory is one of my projects during my VMware years. The main purpose is to automatically and dynamically create nested SDDC environments. It’s means you have the capability to create a separated but nested vSphere SDDC fully routed (dedicated L2 but L3 routed) configured and ready to use.</p>

<p>It comes with <code class="language-plaintext highlighter-rouge">cpodctl</code> command line giving you all necessary command to manage cPod lifecycle. A cPod is a lab or if you prefer the nested vSphere. You can:</p>
<ol>
  <li>Create a cPod: naked one (without esx but all networking facilities) or specifying number of esx</li>
  <li>Delete a cPod entirely, caution - it’s fast without roll back</li>
  <li>Deploy &amp; configure vCenter accordingly your cPod</li>
  <li>List all cPods</li>
  <li>Deploy VCF cloudbuilder if you want to transform cPod into VCF cluster(s)</li>
  <li>Retrieve the auto generated cPod password</li>
  <li>Override some variables to customize your cPod</li>
  <li>Use completion in bash to facilitate life with tab key</li>
</ol>

<p>cpodctl cli:
<img src="/gfx/cpodfactory-cpodctl.png" alt="cpodctl cli" /></p>

<p>A large Availability Zone running physical layer by VCF:
<img src="/gfx/cpodfactory-az-fkd.png" alt="cpodctl cli" /></p>

<p>cPodFactory also offers:</p>
<ul>
  <li>Multi availability zones and connect them via wireguard: it’s possible to move workload across different az or to connect services on different cPods</li>
  <li>DNS resolution for all az</li>
  <li>VPN for client with OpenVPN, Windows/Mac/Linux compatible</li>
  <li>DHCP for each L2 connectivity</li>
  <li>Dynamic L3 routing with BGP, with the possibility to peer NSX Edge in a cPod</li>
  <li>Nested NSX and nested VSAN</li>
  <li>It comes with cPodEdge and cPodRouter VM template for connectivity and network services (based on PhotonOS Linux)</li>
  <li>Utilities to automaticaly deploy OVA and Windows/Linux jumpHost in cPod</li>
</ul>

<p>It needs for the physical layer:</p>
<ul>
  <li>at least 2 or 3 esx 6.7 + vCenter or <a href="https://www.vmware.com/products/cloud-foundation.html">VCF</a> with 4 esx</li>
  <li>vSAN for storage or any shared datastore (NFS, iSCSI, FC)</li>
  <li>VLAN backend network or NSX overlay (NSX is not mandatory at all)</li>
</ul>

<p>I ran it for 4 years to help colleagues to have serious labs and also used it for all my customer workshops. We have never had severe issue of availability or performance - thanks to vSphere for its capabilities to run heavy workloads. It has been deployed in several country interconnected via wireguard: Paris(FR), OVH(FR), Madrid(SP), Munich(DE), Dubai(UAE).</p>

<p>To give you some ideas of use cases:</p>
<ul>
  <li>Large Proof Of Concept with Tanzu running Telco NVFs and consuming +480Gb memory in kubernetes, 9 esx cPod</li>
  <li>Demo cPod dedicated to <a href="https://www.vmware.com/products/vrealize-automation.html">vRealize Automation</a> with NSX and vSAN connected with Public Cloud deploying hybrid App</li>
  <li>multi <a href="https://www.vmware.com/products/cloud-foundation.html">VCF</a> cPods in different regions (Paris &lt;-&gt; Dubai) moving VM workloads via <a href="https://www.vmware.com/products/hcx.html">HCX</a></li>
  <li>Cloud Migration from cPod in <a href="https://www.ovhcloud.com">OVH</a> to <a href="https://vmc.vmware.com/home">VMC</a> in AWS, moving VM via <a href="https://www.vmware.com/products/hcx.html">HCX</a>, fully routed without application interruption, keeping on-prem DB</li>
  <li>All new vSphere 8 cPod with <a href="https://tanzu.vmware.com/">Tanzu</a> Grid 2.0 for training purpose</li>
  <li>nVIDIA AI Enterprise naked cPod bringing agile AI for enterprise running on vSphere</li>
</ul>

<p>This project is open source. You can use it, improve it but not in production. Nested is not supported/allowed for production.</p>

<p><a href="https://github.com/bdereims/cPodFactory">Check out git repo</a> - you find here all the necessary to install yours.</p>

<p>Enjoy!</p>]]></content><author><name></name></author><category term="vmware" /><summary type="html"><![CDATA[cPodFactory is one of my projects during my VMware years. The main purpose is to automatically and dynamically create nested SDDC environments. It’s means you have the capability to create a separated but nested vSphere SDDC fully routed (dedicated L2 but L3 routed) configured and ready to use.]]></summary></entry><entry><title type="html">Tanzu Konw It All</title><link href="https://bdereims.github.io/kubernetes/2022/06/24/tkia.html" rel="alternate" type="text/html" title="Tanzu Konw It All" /><published>2022-06-24T18:59:00+02:00</published><updated>2022-06-24T18:59:00+02:00</updated><id>https://bdereims.github.io/kubernetes/2022/06/24/tkia</id><content type="html" xml:base="https://bdereims.github.io/kubernetes/2022/06/24/tkia.html"><![CDATA[<p>If you want to quickly discover Tanzu or practice a lot kubernetes in order to prepare CKA or CKAD certification, I have thing for you…</p>

<p>Check out my <a href="https://github.com/bdereims/tkia">TKIA</a> git repo, you get here links,docs and usefull scripts that I made during so many customer workshops.</p>

<p>Enjoy!</p>]]></content><author><name></name></author><category term="kubernetes" /><summary type="html"><![CDATA[If you want to quickly discover Tanzu or practice a lot kubernetes in order to prepare CKA or CKAD certification, I have thing for you…]]></summary></entry></feed>